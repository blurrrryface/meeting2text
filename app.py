import streamlit as st
from funasr import AutoModel
from modelscope.pipelines import pipeline
from sencevoice import SenceVoice
from speaker_diarization import SpeakerDiarization
import pandas as pd
from openai import OpenAI

# è®¾ç½®é¡µé¢å¸ƒå±€ä¸ºå®½å±æ¨¡å¼
st.set_page_config(layout="wide")
client = OpenAI(api_key="sk-264bcea8497949f1b6d82365732f6173", base_url="https://api.deepseek.com")


@st.cache_resource
def load_models():
    model = AutoModel(model="fsmn-vad", model_revision="v2.0.4", disable_update=True)
    sv_pipeline = pipeline(
        task='speaker-verification'
        # , model="damo/speech_eres2net_sv_zh-cn_16k-common"
        , model="damo/speech_eres2netv2_sv_zh-cn_16k-common"
        , disable_update=True)
    change_locator_pipeline = pipeline(
        task='speaker-diarization',
        model='damo/speech_xvector_transformer_scl_zh-cn_16k-common', disable_update=True)
    sv_model = AutoModel(
        model="iic/SenseVoiceSmall",
        trust_remote_code=False,
        vad_model="fsmn-vad",
        vad_kwargs={"max_single_segment_time": 30000},
        device="cuda:0",
        disable_update=True
    )

    sencevioce = SenceVoice(sv_model)

    sd = SpeakerDiarization(vad_model=model,
                            sv_pipeline=sv_pipeline,
                            change_locator_pipeline=change_locator_pipeline,
                            sv_model=sencevioce
                            )
    return sd


sd = load_models()

# åˆå§‹åŒ–ä¼šè¯çŠ¶æ€
if 'audio_df' not in st.session_state:
    st.session_state.audio_df = None
if 'speaker_mapping' not in st.session_state:
    st.session_state.speaker_mapping = {}

# Streamlit åº”ç”¨
st.title("ğŸ™ï¸ è¯´è¯äººè¯†åˆ« + ğŸ—£ï¸ è¯­éŸ³è¯†åˆ« + ğŸ¤– LLMå¯¹è¯ ğŸš€")


def format_time(time):
    if time < 60:
        time_str = f"{int(time)} s"
    else:
        minutes = int(time // 60)
        seconds = int(time % 60)
        time_str = f"{minutes} m {seconds} s"
    return time_str


with st.sidebar:
    st.subheader("éŸ³é¢‘æ–‡ä»¶è§£æ")
    uploaded_file = st.file_uploader("ä¸Šä¼ æ–‡ä»¶", type=["mp3", "wav", "flac", "mp4"])
    language = st.selectbox("é€‰æ‹©è¯­è¨€", options=['en', 'zh', 'auto'], index=1)
    speaker_num = st.number_input("å¯¹è¯äººæ•°", min_value=1, max_value=10, value=1)
    interval_num = st.number_input("æŒ‰æ—¶é—´é—´éš”åˆå¹¶çŸ­éŸ³é¢‘ï¼Œä¸º0ä¸åˆå¹¶", min_value=0, max_value=10, value=2)
    if st.button("è§£æ"):
        if uploaded_file is not None:
            st.session_state.audio_df = sd.discern(uploaded_file, language, speaker_num=speaker_num,
                                                   merge_chunk=interval_num)
            st.session_state.speaker_mapping = {}  # é‡ç½®è¯´è¯äººæ˜ å°„
            st.success("æ–‡ä»¶è§£æå®Œæˆï¼")
        else:
            st.error("è¯·å…ˆä¸Šä¼ æ–‡ä»¶ï¼")

    # å¯¹è¯å†…å®¹:
    st.subheader("å¯¹éŸ³é¢‘å†…å®¹è¿›è¡Œæé—®")

    if question := st.chat_input():
        # question = st.input("å…³äºéŸ³é¢‘çš„é—®é¢˜")

        if st.session_state.audio_df is not None and len(question) > 1:
            conversation = "\n".join([
                                         f"{format_time(row['å¼€å§‹æ—¶é—´'])} ~ {format_time(row['ç»“æŸæ—¶é—´'])} | {st.session_state.speaker_mapping.get(row['è¯´è¯äºº'], row['è¯´è¯äºº'])}: {row['éŸ³é¢‘æ–‡æœ¬å†…å®¹']}"
                                         for index, row in st.session_state.audio_df.iterrows()])
            response = client.chat.completions.create(
                model="deepseek-chat",
                messages=[
                    {"role": "system",
                     "content": "ä¸‹é¢æ˜¯ä¸€æ®µéŸ³é¢‘è½¬æ–‡æœ¬åçš„å†…å®¹ï¼Œä½ éœ€è¦æ ¹æ®è¿™æ®µå†…å®¹å›ç­”ç”¨æˆ·æå‡ºæ¥çš„é—®é¢˜,å¹¶ä¸”æœ€åé™„ä¸Šç›¸å…³ç‚¹æ—¶é—´ç‚¹åˆ°ç»“å°¾è¿›è¡Œä½è¯\n====\n" + conversation},
                    {"role": "user", "content": question},
                ],
                temperature=0.3,
                stream=False
            )

            background = response.choices[0].message.content
            st.write("å¯¹è¯ç»“æœï¼š")
            st.markdown(background)
        else:
            st.error("è¯·å…ˆè§£æéŸ³é¢‘æ–‡ä»¶ï¼")

if st.session_state.audio_df is not None and len(st.session_state.audio_df) > 0:
    # æ˜¾ç¤ºè¯´è¯äººæ˜ å°„ä¿®æ”¹æ¡†
    st.subheader("ä¿®æ”¹è¯´è¯äººæ˜ å°„")
    for speaker in st.session_state.audio_df['è¯´è¯äºº'].unique():
        if speaker not in st.session_state.speaker_mapping:
            st.session_state.speaker_mapping[speaker] = speaker
        new_name = st.text_input(f"ä¿®æ”¹è¯´è¯äºº {speaker} çš„åå­—", value=st.session_state.speaker_mapping[speaker])
        st.session_state.speaker_mapping[speaker] = new_name

    # æ˜¾ç¤ºè§£æç»“æœ
    st.subheader("è§£æç»“æœ")
    for index, row in st.session_state.audio_df.iterrows():
        start_time = row['å¼€å§‹æ—¶é—´']
        end_time = row['ç»“æŸæ—¶é—´']
        start_time_str = format_time(start_time)
        end_time_str = format_time(end_time)

        speaker = row['è¯´è¯äºº']
        speaker_name = st.session_state.speaker_mapping[speaker]

        st.write(f"##### {start_time_str} ~ {end_time_str} \n è¯´è¯äºº | **{speaker_name}** : {row['éŸ³é¢‘æ–‡æœ¬å†…å®¹']}")
        st.audio(row['æ–‡ä»¶è·¯å¾„'], format='audio/wav')
else:
    st.info("è¯·å…ˆä¸Šä¼ å¹¶è§£ææ–‡ä»¶ã€‚")

